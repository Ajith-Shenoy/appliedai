{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD Classifier with Logloss implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuGupta5883/appliedai/blob/master/SGD_Classifier_with_Logloss_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11",
        "colab_type": "text"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# please don't change random_state\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31db4316-27ac-4e66-ecef-09021527b82e"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#please don't change random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "456b82cb-69db-4ad6-ec33-aa55db8e09ee"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9cd8bf4a-d7d9-41de-fe61-dbf4e758e2b0"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "f8f74732-0884-4504-8d11-973cf53a1a06"
      },
      "source": [
        "clf.fit(X=X_train, y=y_train) # fitting our model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
            "Total training time: 0.08 seconds.\n",
            "Convergence after 10 epochs took 0.08 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d3136ea8-bdd5-4364-b393-b25137cde904"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
              "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
              "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
              " (1, 15),\n",
              " array([-0.8531383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY",
        "colab_type": "text"
      },
      "source": [
        "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_8bdzitDlM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  We will be giving you some functions, please write code in that functions only.\n",
        "\n",
        "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (dim,1) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "    w = np.zeros_like(dim)\n",
        "    b = 0\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0c0aac7d-dce3-4f3d-c288-6be4dee4e15b"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',str(b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN",
        "colab_type": "text"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44c8fb6a-19f8-4559-91c7-924529a650b8"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs",
        "colab_type": "text"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m",
        "colab_type": "text"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e225a43-9a09-4756-b566-11251af0b7b1"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy",
        "colab_type": "text"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logloss(y_true,y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    loss =0.0\n",
        "    for i in range(len(y_true)):\n",
        "      loss+= (y_true[i]* np.log10(y_pred[i] + 0.000000000000000000000001)) + ((1-y_true[i]) * np.log10(1- y_pred[i]+ 0.000000000000000000000001))\n",
        "\n",
        "    return -loss/float(len(y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt",
        "colab_type": "text"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79f6a57e-dcf0-4ab2-8430-ff46ccd07543"
      },
      "source": [
        "def grader_logloss(true,pred):\n",
        "  loss=logloss(true,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true,pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd",
        "colab_type": "text"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    sigma = sigmoid(np.dot(w.T,x) + b )\n",
        "    dw= x *(y -sigma) - (alpha * w)/N\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9",
        "colab_type": "text"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3853acff-46d1-4788-902b-7b83124d46ce"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N",
        "colab_type": "text"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     '''In this function, we will compute gradient w.r.to b '''\n",
        "     db = y - sigmoid(np.dot(w.T,x) + b) \n",
        "     return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk",
        "colab_type": "text"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6fac211-dc26-4b7d-f4c4-d30b304578b1"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkGtYQWSxVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAdc5ejEZ25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    #Here eta0 is learning rate\n",
        "    #implement the code as follows\n",
        "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "    # for every epoch\n",
        "        # for every data point(X_train,y_train)\n",
        "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "           #compute gradient w.r.to b (call the gradient_db() function)\n",
        "           #update w, b\n",
        "        # predict the output of x_train[for all data points in X_train] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the train loss values in a list\n",
        "        # predict the output of x_test[for all data points in X_test] using w,b\n",
        "        #compute the loss between predicted and actual values (call the loss function)\n",
        "        # store all the test loss values in a list\n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
        "    loss =0\n",
        "    log_loss_tr =[]\n",
        "    log_loss_test =[]\n",
        "\n",
        "    N = len(X_train)\n",
        "    w,b = initialize_weights(X_train[0])\n",
        "    for i in range(epochs):\n",
        "      #print(\"epoch :\",i)\n",
        "      for i,j in zip(X_train,y_train):\n",
        "        grad_w = gradient_dw(i,j,w,b,alpha,N)\n",
        "        grad_b = gradient_db(i,j,w,b)\n",
        "        w+= (eta0 * grad_w)\n",
        "        b+= (eta0 * grad_b)\n",
        "      y_pred_tr = pred(w,b,X_train)\n",
        "      train_log_loss = logloss(y_train,y_pred_tr)\n",
        "      log_loss_tr.append(train_log_loss)\n",
        "      #print(\"train_log_loss :\", train_log_loss)\n",
        "      y_pred_test = pred(w,b,X_test)\n",
        "      test_log_loss = logloss(y_test,y_pred_test)\n",
        "      log_loss_test.append(test_log_loss)\n",
        "      #print(\"test_log_loss :\", test_log_loss)\n",
        "    return w,b,log_loss_tr,log_loss_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cbfea6c1-6185-49e6-99c2-99e46f1b5cf8"
      },
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(X_train)\n",
        "epochs=50\n",
        "#epochs =5\n",
        "w,b,log_loss_tr,log_loss_test=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)\n",
        "print(w,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.42979243  0.1930352  -0.14846994  0.33809364 -0.22128233  0.56994892\n",
            " -0.44518163 -0.08990403  0.2218295   0.17382964  0.19874846 -0.0005843\n",
            " -0.08133411  0.3390901   0.02298796] -0.8922521633477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Zf_wPARlwY",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>Goal of assignment</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eF_VSPSH2z",
        "colab_type": "text"
      },
      "source": [
        "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx8Rs9rfEZ1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0d4751e7-6422-4795-c35f-1f91ff338ca5"
      },
      "source": [
        "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
        "w-clf.coef_, b-clf.intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.00642552,  0.00755955,  0.00012041, -0.00335043, -0.01309563,\n",
              "          0.00978314,  0.00724319,  0.00418409,  0.0125563 , -0.00701162,\n",
              "          0.00169655, -0.00480346, -0.00173041,  0.00056208,  0.00032075]]),\n",
              " array([-0.03911387]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqhdwl6LJSGx",
        "colab_type": "text"
      },
      "source": [
        "# Observation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRwcQy8cJiDU",
        "colab_type": "text"
      },
      "source": [
        "## 1. Difference of weight and intercept is less than 10^-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230YbSgNSUrQ",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
        "\n",
        "* epoch number on X-axis\n",
        "* loss on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6GrRt7UeCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "449a701c-233b-4142-f474-008c7bd44b8b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = range(0,50)\n",
        "plt.plot(epochs,log_loss_tr , 'g', label='Train Log loss')\n",
        "plt.plot(epochs,log_loss_test , 'b', label='Test Log loss') \n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('epoch v/s Logistic log loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CyMZPQrG8mZ",
        "colab_type": "text"
      },
      "source": [
        "# Observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RhPRPFHE0W",
        "colab_type": "text"
      },
      "source": [
        "## 1. There is sharp fall in Test and Train log loss in beginning.\n",
        "## 2. After 21 epoch Test and Train log loss is almost constant.\n",
        "## 3. Train log loss is slightly higher than Test log loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN8puFoEZtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "af18fe26-3447-4733-d135-7c70f3695a39"
      },
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.96512\n",
            "0.96272\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}